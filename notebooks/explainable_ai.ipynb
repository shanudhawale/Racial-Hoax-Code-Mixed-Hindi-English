{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install google-transliteration-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModelForSequenceClassification\n",
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import wandb\n",
    "from transformers.integrations import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "\n",
    "\n",
    "train_df = train_df.drop(columns=['Num_Tokens','Num_Sentences'])\n",
    "test_df = test_df.drop(columns=['Num_Tokens','Num_Sentences'])\n",
    "val_df = val_df.drop(columns=['Num_Tokens','Num_Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': \"final_train.csv\",\n",
    "                                              'val':\"final_val.csv\",'test':\"final_test.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/rh-code-mixed-2.pkl', 'rb') as f:\n",
    "    dict_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_preprocess(sentences, related_words):\n",
    "    texts = sentences['clean_text']\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        process_text = []\n",
    "\n",
    "        for word in words:\n",
    "            # Check if the word exists in the related_words dictionary\n",
    "            if related_words[word].get('corrected_word') != None :\n",
    "                # Append the h_script value from related_words to the interleaved_text list\n",
    "                process_text.append(related_words[word]['corrected_word'])\n",
    "            else:\n",
    "                # If the word is not in related_words, just append the original word\n",
    "                process_text.append(word)\n",
    "\n",
    "        # Join the list back into a string\n",
    "        processed_texts.append(\" \".join(process_text))\n",
    "\n",
    "    return {\"corrected_text\": processed_texts}\n",
    "\n",
    "dataset = dataset.map(\n",
    "    corrected_preprocess,\n",
    "    fn_kwargs={'related_words': dict_words},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-mbert\")\n",
    "\n",
    "mbert_dataset = dataset.map(\n",
    "    lambda example: tokenizer(example['corrected_text'], max_length=97, padding='max_length', truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=64\n",
    ")\n",
    "mbert_dataset = mbert_dataset.remove_columns([\"clean_text\",\"corrected_text\",\"language_tags\"])\n",
    "mbert_dataset.set_format(\"torch\")\n",
    "\n",
    "# Define all possible class labels\n",
    "class_labels = np.unique(mbert_dataset['train']['labels'])\n",
    "\n",
    "# Calculate class weights\n",
    "labels = mbert_dataset['train']['labels']\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=labels.numpy())\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "# Define a custom Trainer class to include class weights\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\").to(model.device)  # Ensure labels are on the same device as model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Ensure all inputs are on the same device as model\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute weighted loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(model.device))  # Move class_weights to the same device as model\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    # Additional metrics\n",
    "    recall_micro = recall_score(labels, predictions, average='micro')\n",
    "    f1_micro = f1_score(labels, predictions, average='micro')\n",
    "\n",
    "    recall_macro = recall_score(labels, predictions, average='macro')\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    recall_positive = recall_score(labels, predictions, pos_label=1)\n",
    "    f1_positive = f1_score(labels, predictions, pos_label=1)\n",
    "\n",
    "    recall_negative = recall_score(labels, predictions, pos_label=0)\n",
    "    f1_negative = f1_score(labels, predictions, pos_label=0)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_weighted': precision,\n",
    "        'recall_weighted': recall,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'recall_positive': recall_positive,\n",
    "        'recall_negative': recall_negative,\n",
    "        'f1_weighted': f1,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_positive': f1_positive,\n",
    "        'f1_negative': f1_negative\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"rh1\", name=\"Hing_mBERT_corrected\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/hing-mbert\", num_labels=2)\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_HingMBert_trainer_5k_corrected_text\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    optim = 'adamw_torch',\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_weighted',  # Define the metric for early stopping\n",
    "    greater_is_better=True,  # Set to False because we want to minimize the loss\n",
    "    save_total_limit=1,\n",
    "    seed=224,\n",
    "    report_to=\"wandb\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=mbert_dataset['train'],\n",
    "    eval_dataset=mbert_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    # will call on_log on each logging step, specified by TrainerArguement. (i.e TrainerArguement.logginng_step)\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.01))\n",
    "trainer.add_callback(LoggingCallback(\"sample_HingMBert_trainer_5k_corrected_text/log.jsonl\"))\n",
    "\n",
    "# train the model\n",
    "trainer.train()\n",
    "\n",
    "mbert_results = trainer.evaluate()\n",
    "\n",
    "test_results_mbert = trainer.predict(mbert_dataset['test'])\n",
    "test_results_mbert.predictions.argmax(axis=1)\n",
    "test_df['corrected'] = test_results_mbert.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliterated Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.transliteration import transliterate_word\n",
    "def transliterated_preprocess(sentences, related_words):\n",
    "    texts = sentences['clean_text']\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        process_text = []\n",
    "\n",
    "        for word in words:\n",
    "            # Check if the word exists in the related_words dictionary\n",
    "            if related_words[word].get('h_script') != None :\n",
    "                # Append the h_script value from related_words to the interleaved_text list\n",
    "                process_text.append(related_words[word]['h_script'])\n",
    "            else:\n",
    "                # If the word is not in related_words, just append the original word\n",
    "                suggestions = transliterate_word(word, lang_code='hi')\n",
    "                process_text.append(suggestions[0])\n",
    "\n",
    "        # Join the list back into a string\n",
    "        processed_texts.append(\" \".join(process_text))\n",
    "\n",
    "    return {\"transliterated_text\": processed_texts}\n",
    "\n",
    "trans_dataset = dataset.map(\n",
    "    transliterated_preprocess,\n",
    "    fn_kwargs={'related_words': dict_words},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_trans_dataset = trans_dataset.map(\n",
    "    lambda example: tokenizer(example['transliterated_text'],max_length=97,padding='max_length', truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=64\n",
    ")\n",
    "mbert_trans_dataset = mbert_trans_dataset.remove_columns([\"clean_text\",\"language_tags\",\"transliterated_text\"])\n",
    "mbert_trans_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/hing-mbert\", num_labels=2)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_HingMBert_trainer_5k_hi_dev\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    optim = 'adamw_torch',\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_weighted',  # Define the metric for early stopping\n",
    "    greater_is_better=True,  # Set to False because we want to minimize the loss\n",
    "    save_total_limit=1,\n",
    "    seed=224,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "\n",
    "trans_trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=mbert_dataset['train'],\n",
    "    eval_dataset=mbert_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.01))\n",
    "trans_trainer.add_callback(LoggingCallback(\"sample_HingMBert_trainer_5k_hi_dev/log.jsonl\"))\n",
    "trans_trainer.train()\n",
    "\n",
    "mbert_trans_results = trans_trainer.evaluate()\n",
    "\n",
    "test_results_mbert = trans_trainer.predict(mbert_trans_dataset['test'])\n",
    "test_results_mbert.predictions.argmax(axis=1)\n",
    "test_df['transliterated'] = test_results_mbert.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transliterated + Skip English Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterated_preprocess(sentences, related_words):\n",
    "    texts = sentences['clean_text']\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        process_text = []\n",
    "\n",
    "        for word in words:\n",
    "            if related_words[word].get('language') == 'HI':\n",
    "                process_text.append(related_words[word]['h_script'])\n",
    "            else:\n",
    "              if related_words[word].get('corrected_word') != None:\n",
    "                process_text.append(related_words[word]['corrected_word'])\n",
    "              else:\n",
    "                process_text.append(word)\n",
    "\n",
    "        # Join the list back into a string\n",
    "        processed_texts.append(\" \".join(process_text))\n",
    "\n",
    "    return {\"transliterated_skip_eng\": processed_texts}\n",
    "\n",
    "trans_skip_dataset = dataset.map(\n",
    "    transliterated_preprocess,\n",
    "    fn_kwargs={'related_words': dict_words},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbert_trans_skip_dataset = trans_skip_dataset.map(\n",
    "    lambda example: tokenizer(example['transliterated_skip_eng'],max_length=97,padding='max_length', truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=64\n",
    ")\n",
    "mbert_trans_skip_dataset = mbert_trans_skip_dataset.remove_columns([\"clean_text\",\"language_tags\",\"transliterated_skip_eng\"])\n",
    "mbert_trans_skip_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_HingMBert_trainer_5k_skip_eng\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    optim = 'adamw_torch',\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_weighted',  # Define the metric for early stopping\n",
    "    greater_is_better=True,  # Set to False because we want to minimize the loss\n",
    "    save_total_limit=1,\n",
    "    seed=224,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trans_skip_trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=mbert_trans_skip_dataset['train'],\n",
    "    eval_dataset=mbert_trans_skip_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_skip_trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.01))\n",
    "trans_skip_trainer.add_callback(LoggingCallback(\"sample_HingMBert_trainer_5k_skip_eng/log.jsonl\"))\n",
    "trans_skip_trainer.train()\n",
    "\n",
    "mbert_trans_skip_results = trans_skip_trainer.evaluate()\n",
    "\n",
    "test_results_mbert = trans_skip_trainer.predict(mbert_trans_skip_dataset['test'])\n",
    "test_results_mbert.predictions.argmax(axis=1)\n",
    "test_df['transliterated_skip'] = test_results_mbert.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaved Language tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleaved_preprocess(sentences):\n",
    "    texts = sentences['clean_text']\n",
    "    tags = sentences['language_tags']\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for text, tag in zip(texts, tags):\n",
    "        words = text.split()\n",
    "        tags_list = tag.split(\", \")\n",
    "        interleaved_text = \" \".join([f\"{related_words[word].get('corrected_word',word)} [{tag}]\" for word, tag in zip(words, tags_list)])\n",
    "        processed_texts.append(interleaved_text)\n",
    "\n",
    "    return {\"interleaved_text_tags\": processed_texts}\n",
    "\n",
    "itags_dataset = dataset.map(interleaved_preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"rh4\", name=\"Hing_mBERT_itag\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/hing-mbert\", num_labels=2)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_HingMBert_trainer_5k_itags\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    optim = 'adamw_torch',\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_weighted',  # Define the metric for early stopping\n",
    "    greater_is_better=True,  # Set to False because we want to minimize the loss\n",
    "    save_total_limit=1,\n",
    "    seed=224,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "\n",
    "itags_trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=itags_dataset['train'],\n",
    "    eval_dataset=itags_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itags_trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.01))\n",
    "itags_trainer.add_callback(LoggingCallback(\"sample_HingMBert_trainer_5k_skip_eng/log.jsonl\"))\n",
    "itags_trainer.train()\n",
    "\n",
    "itags_results = itags_trainer.evaluate()\n",
    "\n",
    "test_results_mbert = itags_trainer.predict(itags_dataset['test'])\n",
    "test_results_mbert.predictions.argmax(axis=1)\n",
    "test_df['transliterated_skip'] = test_results_mbert.predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacent Sentence Language Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacent_preprocess(sentences):\n",
    "    texts = sentences['clean_text']\n",
    "    tags = sentences['language_tags']\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for text, tag in zip(texts, tags):\n",
    "        if isinstance(tag, str):\n",
    "            tag = tag.split(', ')  # Convert a string of tags into a list of tags\n",
    "\n",
    "        tag_string = \" \".join([f\"[{label}]\" for label in tag])\n",
    "        text = \" \".join([f\"{related_words[word].get('corrected_word',word)}\" for word in text.split(\" \")])\n",
    "        adjacent_text = f\"{text} {tag_string}\"\n",
    "        processed_texts.append(adjacent_text)\n",
    "\n",
    "    return {\"adjacent_text_tags\": processed_texts}\n",
    "\n",
    "atags_dataset = dataset.map(adjacent_preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"rh4\", name=\"Hing_mBERT_itag\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/hing-mbert\", num_labels=2)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_HingMBert_trainer_5k_itags\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    optim = 'adamw_torch',\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1_weighted',  # Define the metric for early stopping\n",
    "    greater_is_better=True,  # Set to False because we want to minimize the loss\n",
    "    save_total_limit=1,\n",
    "    seed=224,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "\n",
    "atags_trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=atags_dataset['train'],\n",
    "    eval_dataset=atags_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
